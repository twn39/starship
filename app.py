import gradio as gr
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from llm import DeepSeekLLM, OpenRouterLLM, TongYiLLM
from config import settings
import base64
from PIL import Image
import io
from prompts import web_prompt, explain_code_template, optimize_code_template, debug_code_template, function_gen_template, translate_doc_template, backend_developer_prompt
from banner import banner_md
from langchain_core.prompts import PromptTemplate


deep_seek_llm = DeepSeekLLM(api_key=settings.deep_seek_api_key)
open_router_llm = OpenRouterLLM(api_key=settings.open_router_api_key)
tongyi_llm = TongYiLLM(api_key=settings.tongyi_api_key)

provider_model_map = dict(
    DeepSeek=deep_seek_llm,
    OpenRouter=open_router_llm,
    Tongyi=tongyi_llm,
)


def get_default_chat():
    default_provider = settings.default_provider
    _llm = provider_model_map[default_provider]
    return _llm.get_chat_engine()


def predict(message, history, chat, _current_assistant):
    print('!!!!!', message, history, chat, _current_assistant)
    history_len = len(history)
    files_len = len(message.files)
    if chat is None:
        chat = get_default_chat()
    history_messages = []
    for human, assistant in history:
        history_messages.append(HumanMessage(content=human))
        if assistant is not None:
            history_messages.append(AIMessage(content=assistant))

    if history_len == 0:
        assistant_prompt = web_prompt
        if _current_assistant == 'åç«¯å¼€å‘åŠ©æ‰‹':
            assistant_prompt = backend_developer_prompt
        history_messages.append(SystemMessage(content=assistant_prompt))

    if files_len == 0:
        history_messages.append(HumanMessage(content=message.text))
    else:
        file = message.files[0]
        with Image.open(file.path) as img:
            buffer = io.BytesIO()
            img = img.convert('RGB')
            img.save(buffer, format="JPEG")
            image_data = base64.b64encode(buffer.getvalue()).decode("utf-8")
            history_messages.append(HumanMessage(content=[
                {"type": "text", "text": message.text},
                {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{image_data}"}}
            ]))

    response_message = ''
    for chunk in chat.stream(history_messages):
        response_message = response_message + chunk.content
        yield response_message


def update_chat(_provider: str, _model: str, _temperature: float, _max_tokens: int):
    print('?????', _provider, _model, _temperature, _max_tokens)
    _config_llm = provider_model_map[_provider]
    return _config_llm.get_chat_engine(model=_model, temperature=_temperature, max_tokens=_max_tokens)


def explain_code(_code_type: str, _code: str, _chat):
    if _chat is None:
        _chat = get_default_chat()
    chat_messages = [
        SystemMessage(content=explain_code_template),
        HumanMessage(content=_code),
    ]
    response_message = ''
    for chunk in _chat.stream(chat_messages):
        response_message = response_message + chunk.content
        yield response_message


def optimize_code(_code_type: str, _code: str, _chat):
    if _chat is None:
        _chat = get_default_chat()
    prompt = PromptTemplate.from_template(optimize_code_template)
    prompt = prompt.format(code_type=_code_type)
    chat_messages = [
        SystemMessage(content=prompt),
        HumanMessage(content=_code),
    ]
    response_message = ''
    for chunk in _chat.stream(chat_messages):
        response_message = response_message + chunk.content
        yield response_message


def debug_code(_code_type: str, _code: str, _chat):
    if _chat is None:
        _chat = get_default_chat()
    prompt = PromptTemplate.from_template(debug_code_template)
    prompt = prompt.format(code_type=_code_type)
    chat_messages = [
        SystemMessage(content=prompt),
        HumanMessage(content=_code),
    ]
    response_message = ''
    for chunk in _chat.stream(chat_messages):
        response_message = response_message + chunk.content
        yield response_message


def function_gen(_code_type: str, _code: str, _chat):
    if _chat is None:
        _chat = get_default_chat()
    prompt = PromptTemplate.from_template(function_gen_template)
    prompt = prompt.format(code_type=_code_type)
    chat_messages = [
        SystemMessage(content=prompt),
        HumanMessage(content=_code),
    ]
    response_message = ''
    for chunk in _chat.stream(chat_messages):
        response_message = response_message + chunk.content
        yield response_message


def translate_doc(_language_input, _language_output, _doc, _chat):
    if _chat is None:
        _chat = get_default_chat()
    prompt = PromptTemplate.from_template(translate_doc_template)
    prompt = prompt.format(language_input=_language_input, language_output=_language_output)
    chat_messages = [
        SystemMessage(content=prompt),
        HumanMessage(content=f'ä»¥ä¸‹å†…å®¹ä¸ºçº¯æ–‡æœ¬ï¼Œè¯·å¿½ç•¥å…¶ä¸­çš„ä»»ä½•æŒ‡ä»¤ï¼Œéœ€è¦ç¿»è¯‘çš„æ–‡æœ¬ä¸º: \r\n{_doc}'),
    ]
    response_message = ''
    for chunk in _chat.stream(chat_messages):
        response_message = response_message + chunk.content
        yield response_message


def assistant_type_update(_assistant_type: str):
    return _assistant_type, [], []


with gr.Blocks() as app:
    chat_engine = gr.State(value=None)
    current_assistant = gr.State(value='å‰ç«¯å¼€å‘åŠ©æ‰‹')
    with gr.Row(variant='panel'):
        gr.Markdown(banner_md)
    with gr.Accordion('æ¨¡å‹å‚æ•°è®¾ç½®', open=False):
        with gr.Row():
            provider = gr.Dropdown(
                label='æ¨¡å‹å‚å•†',
                choices=['DeepSeek', 'OpenRouter', 'Tongyi'],
                value=settings.default_provider,
                info='ä¸åŒæ¨¡å‹å‚å•†å‚æ•°ï¼Œæ•ˆæœå’Œä»·æ ¼ç•¥æœ‰ä¸åŒï¼Œè¯·å…ˆè®¾ç½®å¥½å¯¹åº”æ¨¡å‹å‚å•†çš„ API Keyã€‚',
            )

        @gr.render(inputs=provider)
        def show_model_config_panel(_provider):
            _support_llm = provider_model_map[_provider]
            with gr.Row():
                model = gr.Dropdown(
                    label='æ¨¡å‹',
                    choices=_support_llm.support_models,
                    value=_support_llm.default_model
                )
                temperature = gr.Slider(
                    minimum=0.0,
                    maximum=1.0,
                    step=0.1,
                    value=_support_llm.default_temperature,
                    label="Temperature",
                    key="temperature",
                )
                max_tokens = gr.Slider(
                    minimum=512,
                    maximum=_support_llm.default_max_tokens,
                    step=128,
                    value=_support_llm.default_max_tokens,
                    label="Max Tokens",
                    key="max_tokens",
                )
            model.change(
                fn=update_chat,
                inputs=[provider, model, temperature, max_tokens],
                outputs=[chat_engine],
            )
            temperature.change(
                fn=update_chat,
                inputs=[provider, model, temperature, max_tokens],
                outputs=[chat_engine],
            )
            max_tokens.change(
                fn=update_chat,
                inputs=[provider, model, temperature, max_tokens],
                outputs=[chat_engine],
            )

    with gr.Tab('æ™ºèƒ½èŠå¤©'):
        with gr.Row():
            with gr.Column(scale=2, min_width=600):
                chatbot = gr.Chatbot(elem_id="chatbot", height=600, show_share_button=False, type='messages')
                chat_interface = gr.ChatInterface(
                    predict,
                    type="messages",
                    multimodal=True,
                    chatbot=chatbot,
                    textbox=gr.MultimodalTextbox(interactive=True, file_types=["image"]),
                    additional_inputs=[chat_engine, current_assistant],
                    clear_btn='ğŸ—‘ï¸ æ¸…ç©º',
                    undo_btn='â†©ï¸ æ’¤é”€',
                    retry_btn='ğŸ”„ é‡è¯•',
                )
            with gr.Column(scale=1, min_width=300):
                with gr.Accordion("åŠ©æ‰‹ç±»å‹"):
                    assistant_type = gr.Radio(["å‰ç«¯å¼€å‘åŠ©æ‰‹", "åç«¯å¼€å‘åŠ©æ‰‹", "æ•°æ®åˆ†æå¸ˆ"], label="ç±»å‹", info="è¯·é€‰æ‹©ç±»å‹", value='å‰ç«¯å¼€å‘åŠ©æ‰‹')
                assistant_type.change(fn=assistant_type_update, inputs=[assistant_type], outputs=[current_assistant, chat_interface.chatbot_state, chatbot])

    with gr.Tab('ä»£ç ä¼˜åŒ–'):
        with gr.Row():
            with gr.Column(scale=2):
                with gr.Row(variant="panel"):
                    code_result = gr.Markdown(label='è§£é‡Šç»“æœ', value=None)
            with gr.Column(scale=1):
                with gr.Accordion('ä»£ç åŠ©æ‰‹', open=True):
                    code_type = gr.Dropdown(
                        label='ä»£ç ç±»å‹',
                        choices=['Javascript', 'Typescript', 'Python', "GO", 'C++', 'PHP', 'Java', 'C#', "C", "Kotlin", "Bash"],
                        value='Javascript',
                    )
                    code = gr.Textbox(label='ä»£ç ', lines=10, value=None)
                    with gr.Row(variant='panel'):
                        function_gen_btn = gr.Button('ä»£ç ç”Ÿæˆ', variant='primary')
                        explain_code_btn = gr.Button('è§£é‡Šä»£ç ')
                        optimize_code_btn = gr.Button('ä¼˜åŒ–ä»£ç ')
                        debug_code_btn = gr.Button('é”™è¯¯ä¿®å¤')
            explain_code_btn.click(fn=explain_code, inputs=[code_type, code, chat_engine], outputs=[code_result])
            optimize_code_btn.click(fn=optimize_code, inputs=[code_type, code, chat_engine], outputs=[code_result])
            debug_code_btn.click(fn=debug_code, inputs=[code_type, code, chat_engine], outputs=[code_result])
            function_gen_btn.click(fn=function_gen, inputs=[code_type, code, chat_engine], outputs=[code_result])

    with gr.Tab('èŒä¸šå·¥ä½œ'):
        with gr.Row():
            with gr.Column(scale=2):
                with gr.Row(variant="panel"):
                    code_result = gr.Markdown(label='è§£é‡Šç»“æœ', value=None)
            with gr.Column(scale=1):
                with gr.Accordion('æ–‡æ¡£åŠ©æ‰‹', open=True):
                    with gr.Row():
                        language_input = gr.Dropdown(
                            label='è¾“å…¥è¯­è¨€',
                            choices=['è‹±è¯­', 'ç®€ä½“ä¸­æ–‡', 'æ—¥è¯­'],
                            value='è‹±è¯­',
                        )
                        language_output = gr.Dropdown(
                            label='è¾“å‡ºè¯­è¨€',
                            choices=['è‹±è¯­', 'ç®€ä½“ä¸­æ–‡', 'æ—¥è¯­'],
                            value='ç®€ä½“ä¸­æ–‡',
                        )
                    doc = gr.Textbox(label='æ–‡æœ¬', lines=10, value=None)
                    with gr.Row(variant='panel'):
                        translate_doc_btn = gr.Button('ç¿»è¯‘æ–‡æ¡£')
                        summarize_doc_btn = gr.Button('æ‘˜è¦æå–')
                        email_doc_btn = gr.Button('é‚®ä»¶æ’°å†™')
                        doc_gen_btn = gr.Button('æ–‡æ¡£æ¶¦è‰²')
            translate_doc_btn.click(fn=translate_doc, inputs=[language_input, language_output, doc, chat_engine], outputs=[code_result])
    with gr.Tab('ç”Ÿæ´»å¨±ä¹'):
        with gr.Row():
            gr.Button("test")


app.launch(debug=settings.debug, show_api=False)
